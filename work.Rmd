---
title: "Barbell movement analysis"
author: "Alex Gaggin"
date: November 18, 2015
output: pdf_document
---

### Summary

[A dataset](http://groupware.les.inf.puc-rio.br/har)
with movement sensor data from 4 healthy subjects doing 5 classes
of activities (sitting-down, standing-up, standing, walking,
and sitting) was used to apply machine learning algorithms to identify
these activities based on the sensor data. This is a course project
for Practical Machines Leaning course in Data Science specialization on
Coursera. Benchmark prediction accuracy for this dataset is listed as 99.4144%.


```{r, include=FALSE}
# setwd(file.path(normalizePath("~"),"kaggle","predmachlearn-034"))
library(caret)
library(dplyr)
```

We download the training dataset and split it to three parts - 60% training,
20% development test, 20% validation of out-of-sample error. The selected
algorithm will be later re-trained on the full set of training data before
making predictions on the testing part of the dataset that will be downloaded
separately and submitted as a second part of this project assignment.

```{r}
remote <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
loc <- basename(remote)
if(!file.exists(loc )) download.file(remote, loc)
if(!exists("loaded")) loaded <- read.csv(loc)
# Keep original data frame for console work caching and make a mutable copy
working <- loaded
set.seed(1000)
inTrain  <- createDataPartition(working$classe, p = 0.6, list=FALSE)
training <- working[inTrain, ]
nontraining <- working[-inTrain, ]
inTest <- createDataPartition(nontraining$classe, p = 0.5, list=FALSE)
testing  <- nontraining[inTest, ]
validating  <- nontraining[-inTest, ]
````

We look at the data and set aside any columns that contain NAs, hoping that
remaining variables will be sufficient for predictions. Later it's shown
that they are indeed sufficient, but it if wouldn't be, we could have returned
and try to handle NAs more
intelligently - for example, cut these variables to bins and make NAs to be just
another bin.  
Then we also remove non-sensor measurements: line numbers, user names,
timestamps, and unclear factor variable new_window. This leaves us with 53
variables.
```{r, cache=TRUE}
nas<-sapply(training, function(x) sum(is.na(x) | x==""))
trainingNotNA<-training[,names(nas[nas == 0])]
tr<-trainingNotNA[,-(1:6)]
dim(training)
dim(tr)
```

NUM_WINDOWS PREDICTS 99.97%

```{r, cache=TRUE}
set.seed(1)
system.time(mod222<-train(classe~num_window,data=training,method="rf"))
confusionMatrix(predict(mod222, testing),testing$classe)
```

We try fast classification tree first and see if it predicts well. It doesn't -
accuracy is barely higher than a coin flip.

```{r, cache=TRUE}
set.seed(1)
mod<-train(classe~.,data=tr,method="rpart")
confusionMatrix(predict(mod, testing),testing$classe)
```

We run Random Forest on this dataset and with default parameters it gets high
prediction accuracy (better than benchmark), but takes a lot of time to train,
which is hard to work with in this research, especially for knitr paper.
Speed can be improved by limiting number of trees (ntree parameter, which is 500
by default),
although at cost of accuracy. Also, using cross-validation instead of
default bootstrapping as resampling method also improves the speed three times 
while
maintaining basically the same prediction accuracy. We use small number of trees just to
compare speed by resampling method. On the test computer (desktop PC) training
time shrinks from 46 to 15 seconds with repeatedcv method while keeping very
similar confidence interval for prediction accuracy - 98.61%-99.2%.

```{r, cache=TRUE}
set.seed(1)
system.time(mod5rf<-train(classe~., data=tr,method="rf", ntree=5))
confusionMatrix(predict(mod5rf, testing),testing$classe)

set.seed(1)
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2)
system.time(mod5rcv<-train(classe~., data=tr,method="rf", ntree=5, trControl=ctrl))
confusionMatrix(predict(mod5rcv, testing),testing$classe)
```

We also see that using just single cross-validation is faster, but costs in
accuracy - it falls to 97.81%-98.65%. Increasing number of repeats doesn't
improve it. Increasing number of folds to 10 (default) increases accuracy
to 98.94%-99.5%.

```{r, cache=TRUE}
set.seed(1)
ctrl <- trainControl(method="cv", number=5)
system.time(mod5cv<-train(classe~., data=tr,method="rf", ntree=5, trControl=ctrl))
confusionMatrix(predict(mod5cv, testing),testing$classe)

set.seed(1)
ctrl <- trainControl(method="repeatedcv", number=5, repeats=5)
system.time(mod5rcv5<-train(classe~., data=tr,method="rf", ntree=5, trControl=ctrl))
confusionMatrix(predict(mod5rcv5, testing),testing$classe)

set.seed(1)
ctrl <- trainControl(method="repeatedcv", number=10, repeats=2)
system.time(mod5rcv10<-train(classe~., data=tr,method="rf", ntree=5, trControl=ctrl))
confusionMatrix(predict(mod5rcv10, testing),testing$classe)
```

At 150 trees with 5 folds and 2 repeats it takes 6 minutes on a desktop PC
to train it with prediction accuracy interval 99.43%-99.82% - still better than
benchmark.


```{r, cache=TRUE}
set.seed(1)
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2,
                           verboseIter=TRUE)
system.time(mod150<-train(classe~., data=tr,method="rf", ntree=150, trControl=ctrl))
confusionMatrix(predict(mod150, testing),testing$classe)
```

We try to further improve speed while keeping accuracy by reducing number of
features - we only keep ones that are most
correlated with outcomes. We select three features that are most
correlated with each of five predicted classes (each will be converted to
separate 0/1 variable to calculate numeric correlations), and get 12
correlated variables.

```{r, cache=TRUE}
nume<-tr[,1:53]
nume$A <- ifelse(tr$classe == "A", 1, 0)
nume$B <- ifelse(tr$classe == "B", 1, 0)
nume$C <- ifelse(tr$classe == "C", 1, 0)
nume$D <- ifelse(tr$classe == "D", 1, 0)
nume$E <- ifelse(tr$classe == "E", 1, 0)
nume.corr <- cor(nume)
corA <- nume.corr[54,1:53] %>% sort(decr=TRUE) %>% head(3) %>% names
corB <- nume.corr[55,1:53] %>% sort(decr=TRUE) %>% head(3) %>% names
corC <- nume.corr[56,1:53] %>% sort(decr=TRUE) %>% head(3) %>% names
corD <- nume.corr[57,1:53] %>% sort(decr=TRUE) %>% head(3) %>% names
corE <- nume.corr[58,1:53] %>% sort(decr=TRUE) %>% head(3) %>% names
(correlated <- unique(c(corA, corB, corC, corD, corE)))
tr12 <- tr[, correlated]
tr12$classe <- tr$classe
```

It looks like execution time almost halved this way, while prediction accuracy
is similar. Supposedly Random Forest automatically discovers same feature
importance.

```{r}
set.seed(1)
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2)
system.time(mod5rcv5.12<-train(classe~., data=tr,method="rf", ntree=5, trControl=ctrl))
confusionMatrix(predict(mod5rcv5.12, testing),testing$classe)
# 15 sec (0.9861, 0.9927)
```

Another approach is to check if multiple sensor are even needed. What if we
predict just based on belt sensors?

```{r}
set.seed(1)
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2)
system.time(mod<-train(classe~., data=tr[, c(1,54)], method="rf", ntree=5, trControl=ctrl))
confusionMatrix(predict(mod, testing),testing$classe)
# 15 sec (0.9861, 0.9927)
```

To try another model,
we use GBM (Generalized Boosted Regression Modeling) method with low number of
trees to predict. We change number of trees and number of available features
to compare speed and accuracy.

```{r, cache=TRUE}
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2, verboseIter=TRUE)
set.seed(1)
system.time(fitGBM5 <- train(classe~.,data=tr[,c(1:5,54)],
                        method="gbm", trControl=ctrl))
confusionMatrix(predict(fitGBM5, testing), testing$classe)
set.seed(1)
system.time(fitGBM50 <- train(classe~.,data=tr[,2:54],
                        method="gbm", trControl=ctrl))
confusionMatrix(predict(fitGBM50, testing), testing$classe)
set.seed(1)
system.time(fitGBM <- train(classe~.,data=tr12,
                        method="gbm", n.trees=5, trControl=ctrl))
confusionMatrix(predict(fitGBM, testing), testing$classe)
# failed
```

It is much faster - takes 37 seconds to train on a
desktop PC and we get prediction accuracy
in the interval 99.15%-99.65% which is very close to benchmark set for this
dataset.

We use Random Forest to improve weak trees and find that with default settings
accuracy is high: 99.68-99.93%.



Try to use less variables to make it work faster

```{r}
system.time(mod42<-train(classe~., data=tr[,c(2,3,4,5,55)],method="rf", ntree=3))
confusionMatrix(predict(mod42, testing),testing$classe)
# 12 sec,  99.6-99.89%
```

But why these four?

```{r}
pps<-prcomp(tr[, -c(1,55)], scale=T)
sort(pps$rotation[,1]) %>% tail(4) %>% names -> impacting
mod47<-train(classe~., data=tr[,c(impacting,"classe")],method="rf", ntree=3)
confusionMatrix(predict(mod47, testing),testing$classe)
# Still only 63%
```

Try to scale
```{r}
scaled <- sapply(tr[,-c(1,55)],scale) %>% as.data.frame
tr5<-scaled
tr5$classe<-tr$classe
```

Try neuralnet

```{r}
tr6<-tr[,-c(1,55)]
ftml<-xyform("cl",names(tr6))
tr6$cl<-ifelse(tr$classe=="A",1,0)
nnA<-neuralnet(ftml,tr6,hidden=5,lifesign="full",thre=0.7)
tr6$cl<-ifelse(tr$classe=="B",1,0)
nnB<-neuralnet(ftml,tr6,hidden=5,lifesign="full",thre=0.7)
tr6$cl<-ifelse(tr$classe=="C",1,0)
nnC<-neuralnet(ftml,tr6,hidden=5,lifesign="full",thre=0.5)
tr6$cl<-ifelse(tr$classe=="D",1,0)
nnD<-neuralnet(ftml,tr6,hidden=5,lifesign="full",thre=0.7)
tr6$cl<-ifelse(tr$classe=="E",1,0)
nnE<-neuralnet(ftml,tr6,hidden=5,lifesign="full",thre=0.7)

testingNotNA<-testing[,names(nas[nas == 0])]
ts<-testingNotNA[,-(1:6)]
ts6<-ts[,-c(54)]
#ts6$cl<-ifelse(ts$classe=="A",1,0)
#res<-ifelse(neuralnet::compute(nn, ts6)$net.result >0.5,1,0)
#confusionMatrix(res,ts6$cl)
# 82% of determining if it's A
#isA<-neuralnet::compute(nnA, ts6)$net.result
#isB<-neuralnet::compute(nnB, ts6)$net.result
#isC<-neuralnet::compute(nnC, ts6)$net.result
#isD<-neuralnet::compute(nnD, ts6)$net.result
#isE<-neuralnet::compute(nnE, ts6)$net.result
isA<-scale(neuralnet::compute(nnA, ts6)$net.result)
isB<-scale(neuralnet::compute(nnB, ts6)$net.result)
isC<-scale(neuralnet::compute(nnC, ts6)$net.result)
isD<-scale(neuralnet::compute(nnD, ts6)$net.result)
isE<-scale(neuralnet::compute(nnE, ts6)$net.result)
projs<-cbind(isA,isB,isC,isD,isE)
projs<-as.data.table(projs)
setnames(projs,names(projs),c("A","B","C","D","E"))
rrr<-matrix(NA,ncol=1)
for(i in 1:length(projs$A)) rrr<-cbind(rrr,names(which.max(projs[i,])))
#table(rrr[-1],ts$classe)
confusionMatrix(rrr[-1],ts$classe)
# Why C sensitivity is sooo low?
# 40%
confusionMatrix(ifelse(isA >0,1,0),ifelse(ts$classe=="A",1,0))
# 80 %
confusionMatrix(ifelse(isB >0,1,0),ifelse(ts$classe=="B",1,0))
# 81 %
confusionMatrix(ifelse(isC >0,1,0),ifelse(ts$classe=="C",1,0))
# 81 %
```

```{r, eval=FALSE}
unique(training[training$user_name=="jeremy","num_window"])
print(qplot(x=training$X,y=training$num_window,color=training$classe))
```
