---
title: "Application of machine learning algorithms to weightlifting sensor data"
author: "Alex Gaggin"
date: "November 18, 2015"
output: html_document
---

```{r package_options, include=FALSE}
knitr::opts_knit$set(message = FALSE, warning = FALSE, cache = TRUE)
```

### Summary

In this research a dataset of sensor readings from subjects performing
a weightlifting exercise in correct and flawed techniques is analyzed. The goal
is to see if machine learning algorithms can be used to recognize exercise
techniques based on sensor data. It's shown that Random Forest training
algorithm with  out-of-bag resampling method produces the best results both
in predictiona accuracy and training time. After reducing dataset to 52
original sensor reading predictors and training with 500 trees,
validation on out-of-sample group of data
point resulted in 99.52% prediction accuracy (95% confidence interval
for prediction accuracy is 99.24-99.71% - better than accuracy listed in
original research paper on this dataset, which was 98.03%).

This exercise was also done to perform prediction as an assignment in Practical
Machine Learning class in Data Science specialization on Coursera. Assignment's
testing dataset included not just the sensor readings, but also an observation
id that allows to calculate outcomes without using machine learning. These
outcomes were also used for verification of machine learning predictions based
on sensors, and they match ideally. 

### Exploratory analysis

[A dataset](http://groupware.les.inf.puc-rio.br/har) (see Appendix 1)
analyzed in this paper
includes sensor data from six subjects executing same weightlifting excersize
in five styles, only one of which is correct. It's shown how machine learning
can be used two interpret sensor data to recognize exercise styles. Practical
application for this kind of technology would be a programmatic trainer that
advises a trainee to correct their exercise form.
This particular paper though is done as a course project for Practical Machine
Learning class
in Data Science specialization on Coursera and is focused on applying machine
learning methods to the dataset.

```{r, include=FALSE}
# setwd(file.path(normalizePath("~"),"kaggle","predmachlearn-034"))
library(caret)
library(dplyr)
library(gridExtra)
library(randomForest)
library(rpart)
```

We download the training dataset and split it to three parts - 60% training,
20% development test, 20% validation of out-of-sample error. The model selection
work will be done with training and test parts, and then once model approach
is settled, the algorithm will be trained on merged train and test subsets
and checked against validation subset (set aside until that point) to assess
out-of-sample prediction accuracy.

```{r, cache=TRUE}
remote <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
dl.read <- function(remote) {
        loc <- basename(remote)
        if(!file.exists(loc )) download.file(remote, loc)
        read.csv(loc)}
working <- dl.read(remote)
set.seed(1000)
inTrain  <- createDataPartition(working$classe, p = 0.6, list=FALSE)
training <- working[inTrain, ]
nontraining <- working[-inTrain, ]
inTest <- createDataPartition(nontraining$classe, p = 0.5, list=FALSE)
testing  <- nontraining[inTest, ]
validating  <- nontraining[-inTest, ]
dim(working)
dim(training)
dim(testing)
dim(validating)
````

But visually reviewing data via View(), it can be seen that the data is grouped
to windows identified by common "num_window" variable
where the last entry has new_window==yes and then it has
more non-empty variables associated with it then observations with
new_window==no.These
ones have a lot of NAs, and "Div/0!", and empty values instead.

We select a short example of the data to illustrate this structure here.
We keep in mind that in the training set some rows are taken out for
verification sub-sets while selecting it, and remember that so the actual window
is probably longer, and there are more variables that shown in the example.

```{r, cache=TRUE}
finished <- training[training$new_window == "yes", "num_window"]
have.prev <- finished[((finished-1) %in% finished)]
length <- training[training$num_window %in% have.prev,
                                "num_window"] %>% table %>% .[.>1] %>% "["(1)
last.obs <- training %>% with(new_window == "yes" &
                                      num_window == names(length)) %>% which
range.obs <- (last.obs - length - 1):(last.obs + 1)
training[range.obs,c(6,7,8,12,18,160)]
```

Because they go last, we assume that the last observation in each window contains
some aggregate information for the whole window. Variable names agree to this
assumption. Supposedly actual application (programmatic trainer) could rely
on this aggregates for decisions, but for the purpose of this paper we ignore
these additional variables and will only keep ones that have values for every
observation.

By the way, all variables of each window are identified  as the same class
(calculation to verify this statement is shown below). Apparently,
because each window is an example of particular weightlifting exercise form.

```{r, cache=TRUE}
# A cross table of window number vs response class, only one class for each
# window has values, and four others are zeroes.
training$num_window %>% table(training$classe) %>% "=="(0) %>% rowSums %>%
        "!="(4) %>% sum

larger <- training %>% with(qplot(X, num_window, color=classe,
    main="Observation grouping per prediction\n class in the training sub-set"))
smaller <- training[training$X %in% 5050:5150, ] %>%
    with(qplot(X, num_window, color=classe,
    main="Zoomed: each point is actually\n a horizonal row of points"))
grid.arrange(larger, smaller, ncol=2)
```

Thus, as Coursera's assignment provides in its testing set the num_window
variable for the same users in the same range (so, from the same experiment),
we can get answers without using machine learning, but with direct lookup by
window number instead. We get these with the goal to use them as verification
for machine learning predictions based on sensor data.

```{r, cache=TRUE}
remote <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
assignment <- dl.read(remote)
# We use the full original training data set just in case of caret completely
# missing some window in the training sample after partition
classe.by.window <- function(window) {
        working$classe[which(working$num_window == window)[1]]}
(answers <- sapply(assignment$num_window,classe.by.window))
````

### Data cleanup

We now also remove non-sensor measurements: line numbers, user names,
timestamps and window markers, and aggreate variables. We are left with 52
predictor variables.

```{r, cache=TRUE}
nas<-sapply(training, function(x) sum(is.na(x) | x == "" | x == "#DIV/0!"))
trainingNotNA<-training[,names(nas[nas == 0])]
tr<-trainingNotNA[,-(1:7)]
dim(tr)
```

### Training with Recursive Partitioning

Because we have categorical outcomes, we can't use linear models, but turn
to classification trees instead.
We try to build simple decision tree and see that it fails to provide
a prediction much better than a coin flip. We use accuracy rate and training
time as two parameters we are interested in.

```{r, cache=TRUE}
set.seed(1)
system.time(
        confusionMatrix(
                predict(train(classe~., data=tr, method="rpart"), testing),
                testing$classe)$overall[1] %>% print)
```

### Training with Random Forest

As simple tree failed, we now try to use Random Forest as an algorithm
that is known for prediction accuracy. Because it's also known to be slow,
we first try it with low number of trees to speed the check up. We see that
accuracy is quite high, and so with higher number of trees we expect to get
result close to the benchmark.

```{r, cache=TRUE}
set.seed(1)
try.rf <- function(ctl, trees=5) {
        system.time(
                mod <- train(classe~., data=tr, method="rf",
                             ntree=trees, trControl=ctl)) %>% print       
        confusionMatrix(predict(mod, testing),
                        testing$classe)$overall[1] %>% print
        return(mod)
        }
trainControl() %>% try.rf -> mod5
```

### Optimizing resampling for the Random Forest

Before running the long training, we try to optimize different parameters
for train control. For the speed, we keep
number of trees low. We realize that we don't know if the comparison results
depend on number of trees, but for the speed we presume they are.

After running tests with various train control methods, we find "repeatedcv",
"adaptive_boot", and "adaptive_LGOCV" methods to work better than others in
terms of accuracy/speed balance, but the best method is "out-of-bag" - it
provides good accuracy,  while being dramatically faster, and its accuracy
to increase in number of trees stronger. Particular measurements are shown
in Appendix 2.

### Optimizing by reduction of predictors

We try to improve accuracy/speed balance by reducing number of predictors
in three ways: by using Principal Component Analysis, by manually selecting
predictors most correlated with outcomes, and by modeling by each of four
sensors separately. See Appendix 3 for the code. Neither of these leads to 
improvement, so we infer that all sensors are needed to correcly identify
exercise techniques.

### Building final model

We now merge training and testing subsets, and then use this wider dataset
to train the final model with Random Forest at high number of trees and with
previously found cross-validation parameters (oob). We apply then resulting
model to validation dataset to infer out-of-sample accuracy assessment.

```{r, cache=TRUE}
training2 <- rbind(training, testing)
nas2 <- sapply(training2, function(x) sum(is.na(x) | x == "" | x == "#DIV/0!"))
trainingNotNA2 <- training2[, names(nas2[nas2 == 0])]
tr2 <- trainingNotNA2[, -(1:7)]

set.seed(1)
system.time(
        mod500oob <- train(classe~., data=tr2, method="rf",
                        ntree=500, trControl=trainControl(method="oob"))
                        ) %>% print

confusionMatrix(predict(mod500oob,validating),validating$classe)
```

Comparison to answers derived from num_window shows full match.

```{r, cache=TRUE}
confusionMatrix(predict(mod500oob, assignment), answers)$overall[1]
```

We check the most importent predictors as identified by the final model and
see that they don't fully match the list of the most correlated in Appendix 3.

```{r, cache=TRUE}
varImpPlot(mod500oob$finalModel, main="Most influencial sensor readings")
```

### Appendices

#### Appendix 1. References

Dataset source is  
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

#### Appendix 2. Trials of various resampling methods

```{r, cache=TRUE}
set.seed(1)
trainControl(method="repeatedcv", number=5, repeats=2) %>% try.rf -> mod5rcv5.2

set.seed(1)
trainControl(method="repeatedcv", number=10, repeats=2) %>% try.rf -> mod5rcv10.2

set.seed(1)
trainControl(method="repeatedcv", number=5, repeats=5) %>% try.rf -> mod5rcv5.5

set.seed(1)
trainControl(method="repeatedcv", number=20, repeats=2) %>% try.rf -> mod5rcv20.2

set.seed(1)
trainControl(method="cv", number=5) %>% try.rf -> mod5cv5

set.seed(1)
trainControl(method="oob") %>% try.rf -> mod5oob

set.seed(1)
trainControl(method="boot632") %>% try.rf -> mod5b6

set.seed(1)
trainControl(method="adaptive_boot") %>% try.rf -> mod5ab

set.seed(1)
trainControl(method="adaptive_cv") %>% try.rf -> mod5acv

set.seed(1)
trainControl(method="LGOCV") %>% try.rf -> mod5lgocv

set.seed(1)
trainControl(method="adaptive_LGOCV") %>% try.rf -> mod5algocv

set.seed(1)
trainControl(method="oob") %>% try.rf(10) -> mod10oob

trainControl(method="adaptive_boot") %>% try.rf(10) -> mod10b

trainControl(method="adaptive_LGOCV") %>% try.rf(10) -> mod10algocv

ctl <- trainControl(method="oob")

```

Just out of curiocity, we check the fastest model against our answers and see that
it already performs well.
```{r, cache=TRUE}
confusionMatrix(predict(mod5oob, assignment), answers)$overall[1]
```

#### Appendix 3. Reduction of predictors attempts

We try to use reduction of predictors via  Principal Component pre-processing
Analysis and see that PCA doesn't improve either accuracy or speed.

```{r, cache=TRUE}
set.seed(1)
system.time(
        mod5PCA <- train(classe~., data=tr, method="rf",
                        ntree=5, trControl=ctl, preProcess="pca", thresh=0.8)
                        ) %>% print       
confusionMatrix(predict(mod5PCA, testing),
                        testing$classe)$overall[1] %>% print
```

We try manual selection of predictors with
highest correlation to outcomes (after making the outcomes separate 0/1
numeric variables). We see that thes increases speed but at a great
accuracy loss. We try to use speed gain to allow for more trees and folds
but it doesn't improve accuracy/speed balance either.

```{r, cache=TRUE}
nume<-tr[,1:52]
nume$A <- ifelse(tr$classe == "A", 1, 0)
nume$B <- ifelse(tr$classe == "B", 1, 0)
nume$C <- ifelse(tr$classe == "C", 1, 0)
nume$D <- ifelse(tr$classe == "D", 1, 0)
nume$E <- ifelse(tr$classe == "E", 1, 0)
nume.corr <- cor(nume)
corA <- nume.corr[53,1:52] %>% sort(decr=TRUE) %>% head(3) %>% names
corB <- nume.corr[54,1:52] %>% sort(decr=TRUE) %>% head(3) %>% names
corC <- nume.corr[55,1:52] %>% sort(decr=TRUE) %>% head(3) %>% names
corD <- nume.corr[56,1:52] %>% sort(decr=TRUE) %>% head(3) %>% names
corE <- nume.corr[57,1:52] %>% sort(decr=TRUE) %>% head(3) %>% names
(correlated <- unique(c(corA, corB, corC, corD, corE)))
tr12 <- tr[, correlated]
tr12$classe <- tr$classe

set.seed(1)
system.time(
        mod12 <- train(classe~., data=tr12, method="rf",
                        ntree=5, trControl=ctl)
                        ) %>% print       
confusionMatrix(predict(mod12, testing),
                        testing$classe)$overall[1] %>% print

set.seed(1)
system.time(
        mod12.20 <- train(classe~., data=tr12, method="rf",
                        ntree=20, trControl=ctl)
                        ) %>% print       
confusionMatrix(predict(mod12.20, testing),
                        testing$classe)$overall[1] %>% print
```

Another approach is to check if multiple sensor are even needed. We see that
while belt sensor data seems to provide better accuracy overall, but
any single sensor is worse than all of the together, and worse than predictors
selected by highest correlations to outcomes.

```{r, cache=TRUE}
tr.belt <- tr[, grepl("belt",names(tr),fixed=TRUE)]
tr.belt$classe <- tr$classe
set.seed(1)
system.time(
        mod.belt <- train(classe~., data=tr.belt, method="rf",
                        ntree=5, trControl=ctl)
                        ) %>% print       
confusionMatrix(predict(mod.belt, testing),
                        testing$classe)$overall[1] %>% print

tr.forearm <- tr[, grepl("forearm",names(tr),fixed=TRUE)]
tr.forearm$classe <- tr$classe
set.seed(1)
system.time(
        mod.forearm <- train(classe~., data=tr.forearm, method="rf",
                        ntree=5, trControl=ctl)
                        ) %>% print       
confusionMatrix(predict(mod.forearm, testing),
                        testing$classe)$overall[1] %>% print

tr.arm <- tr[, grepl("_arm",names(tr),fixed=TRUE)]
tr.arm$classe <- tr$classe
set.seed(1)
system.time(
        mod.arm <- train(classe~., data=tr.arm, method="rf",
                        ntree=5, trControl=ctl)
                        ) %>% print       
confusionMatrix(predict(mod.arm, testing),
                        testing$classe)$overall[1] %>% print

tr.dumbbell <- tr[, grepl("dumbbell",names(tr),fixed=TRUE)]
tr.dumbbell$classe <- tr$classe
set.seed(1)
system.time(
        mod.dumbbell <- train(classe~., data=tr.dumbbell, method="rf",
                        ntree=5, trControl=ctl)
                        ) %>% print       
confusionMatrix(predict(mod.dumbbell, testing),
                        testing$classe)$overall[1] %>% print
```
