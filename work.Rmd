---
title: "Barbell movement analysis"
author: "Alex Gaggin"
date: November 18, 2015
output: pdf_document
---

### Summary

[A dataset](http://groupware.les.inf.puc-rio.br/har) analyzed in this paper
includes sensor data from six subjects executing same weightlifting excersize
in five styles, only one of which is correct. It's shown how machine learning
can be used two interpret sensor data to recognize exercise styles. Practical
application for this kind of technology would be a programmatic trainer that
advises a trainee to correct their exercise form.
This particular paper though is done as a course project for Practical Machine
Learning class
in Data Science specialization on Coursera and is focused on applying machine
learning methods to the dataset.

While it will be shown that we do not need machine learning algorithms to generate
correct prediction for testing dataset provided by Coursera, sensor data
can still be used to recognize weightlifting exercise styles with high degree
of accuracy, and computing time for the learning process required to reach
benchmark accuracy rate (the original article quotes 98.03%) is quite short.

### Exploratory analysis

```{r, include=FALSE}
# setwd(file.path(normalizePath("~"),"kaggle","predmachlearn-034"))
library(caret)
library(dplyr)
library(gridExtra)
```

We download the training dataset and split it to three parts - 60% training,
20% development test, 20% validation of out-of-sample error. The selected
algorithm will be later re-trained on the full set of training data before
making predictions on the testing part of the dataset that will be downloaded
separately and submitted as a second part of this project assignment.

```{r}
remote <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
dl.read <- function(remote) {
        loc <- basename(remote)
        if(!file.exists(loc )) download.file(remote, loc)
        read.csv(loc)}
working <- dl.read(remote)
set.seed(1000)
inTrain  <- createDataPartition(working$classe, p = 0.6, list=FALSE)
training <- working[inTrain, ]
nontraining <- working[-inTrain, ]
inTest <- createDataPartition(nontraining$classe, p = 0.5, list=FALSE)
testing  <- nontraining[inTest, ]
validating  <- nontraining[-inTest, ]
dim(working)
dim(training)
dim(testing)
dim(validating)
````

But visually reviewing data via View(), it can be seen that the data is grouped
to windows identified by common "num_window" variable
where the last entry has new_window==yes and then it has
more non-empty variables associated with it then observations with
new_window==no.These
ones have a lot of NAs, and "Div/0!", and empty values instead.

We select a short example of the data to illustrate this structure here.
We keep in mind that in the training set some rows are taken out for
verification sub-sets while selecting it, and remember that so the actual window
is probably longer, and there are more variables that shown in the example.

```{r}
finished <- training[training$new_window == "yes", "num_window"]
have.prev <- finished[((finished-1) %in% finished)]
length <- training[training$num_window %in% have.prev,
                                "num_window"] %>% table %>% .[.>1] %>% "["(1)
last.obs <- training %>% with(new_window == "yes" &
                                      num_window == names(length)) %>% which
range.obs <- (last.obs - length - 1):(last.obs + 1)
training[range.obs,c(1,2,6,7,8,12,17,18,160)]
```

Because they go last, we assume that the last observation in each window contains
some aggregate information for the whole window. Variable names agree to this
assumption. Supposedly actual application (programmatic trainer) could rely
on this aggregates for decisions, but for the purpose of this paper we ignore
these additional variables and will only keep ones that have values for every
observation.

By the way, all variables of each window are identified  as the same class (calculation
to verify this statement is shown below). Apparently,
because each window is an example of particular weightlifting exercise form.

```{r}
# A cross table of window number vs response class, only one class for each
# window has values, and four others are zeroes.
training$num_window %>% table(training$classe) %>% "=="(0) %>% rowSums %>%
        "!="(4) %>% sum

larger <- training %>% with(qplot(X, num_window, color=classe,
    main="Observation grouping per prediction\n class in the training sub-set"))
smaller <- training[training$X %in% 5050:5150, ] %>%
    with(qplot(X, num_window, color=classe,
    main="Zoomed: each point is actually\n a horizonal row of points"))
grid.arrange(larger, smaller, ncol=2)
```

Thus, as Coursera's assignment provides in its testing set the num_window
variable for the same users in the same range (so, from the same experiment),
we can get answers without using machine learning, but with direct lookup by
window number instead. We get these with the goal to use them as verification
for machine learning predictions based on sensor data.

```{r}
remote <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
assignment <- dl.read(remote)
# We use the full original training data set just in case of caret completely
# missing some window in the training sample after partition
classe.by.window <- function(window) {
        working$classe[which(working$num_window == window)[1]]}
(answers <- sapply(assignment$num_window,classe.by.window))
````

### Data cleanup

We now also remove non-sensor measurements: line numbers, user names,
timestamps and window markers, and aggreate variables. We are left with 52
predictor variables.

```{r, cache=TRUE}
nas<-sapply(training, function(x) sum(is.na(x) | x == "" | x == "#DIV/0!"))
trainingNotNA<-training[,names(nas[nas == 0])]
tr<-trainingNotNA[,-(1:7)]
dim(tr)
```

### Training with Recursive Partitioning

We try to build simple decision tree and see that it fails to provide
a prediction much better than a coin flip. We use accuracy rate and training
time as two parameters we are interested in.

```{r, cache=TRUE}
set.seed(1)
system.time(
        confusionMatrix(
                predict(train(classe~., data=tr, method="rpart"), testing),
                testing$classe)$overall[1] %>% print)
```

### Training with Random Forest

As simple tree failed, we now try to use Random Forest as an algorithm
that is known for prediction accuracy. Because it's also known to be slow,
we first try it with low number of trees to speed the check up. We see that
accuracy is quite high, and so with higher number of trees we expect to get
result close to the benchmark.

```{r, cache=TRUE}
set.seed(1)
try.rf <- function(ctl, trees=5) {
        system.time(
                mod <- train(classe~., data=tr, method="rf",
                             ntree=trees, trainControl=ctl)) %>% print       
        confusionMatrix(predict(mod, testing),
                        testing$classe)$overall[1] %>% print
        return(mod)
        }
trainControl() %>% try.rf -> mod5
```

Before running the long analysis, we try to optimize different parameters
of the training. First, we try using cross-validation instead of
default bootstrapping as resampling method and see that it improves the speed
three times while
maintaining basically the same prediction accuracy. For the speed, we keep
number of trees low. We realize that we don't know if the comparison results
depend on number of trees, but for the speed we presume they are.

```{r, cache=TRUE}
set.seed(1)
trainControl(method="repeatedcv", number=5, repeats=2) %>% try.rf -> mod5rcv5.2

set.seed(1)
trainControl(method="repeatedcv", number=10, repeats=2) %>% try.rf -> mod5rcv10.2

set.seed(1)
trainControl(method="repeatedcv", number=5, repeats=5) %>% try.rf -> mod5rcv5.5

set.seed(1)
trainControl() %>% try.rf(20) -> mod20



set.seed(1)
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2)
system.time(mod5rcv<-train(classe~., data=tr,method="rf", ntree=5, trControl=ctrl))
confusionMatrix(predict(mod5rcv, testing),testing$classe)
```

We also see that using just single cross-validation is faster, but costs in
accuracy - it falls to 97.81%-98.65%. Increasing number of repeats doesn't
improve it. Increasing number of folds to 10 (default) increases accuracy
to 98.94%-99.5%.

```{r, cache=TRUE}
set.seed(1)
ctrl <- trainControl(method="cv", number=5)
system.time(mod5cv<-train(classe~., data=tr,method="rf", ntree=5, trControl=ctrl))
confusionMatrix(predict(mod5cv, testing),testing$classe)

set.seed(1)
ctrl <- trainControl(method="repeatedcv", number=5, repeats=5)
system.time(mod5rcv5<-train(classe~., data=tr,method="rf", ntree=5, trControl=ctrl))
confusionMatrix(predict(mod5rcv5, testing),testing$classe)

set.seed(1)
ctrl <- trainControl(method="repeatedcv", number=10, repeats=2)
system.time(mod5rcv10<-train(classe~., data=tr,method="rf", ntree=5, trControl=ctrl))
confusionMatrix(predict(mod5rcv10, testing),testing$classe)
```

At 150 trees with 5 folds and 2 repeats it takes 6 minutes on a desktop PC
to train it with prediction accuracy interval 99.43%-99.82% - still better than
benchmark.


```{r, cache=TRUE}
set.seed(1)
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2,
                           verboseIter=TRUE)
system.time(mod150<-train(classe~., data=tr,method="rf", ntree=150, trControl=ctrl))
confusionMatrix(predict(mod150, testing),testing$classe)
```

We try to further improve speed while keeping accuracy by reducing number of
features - we only keep ones that are most
correlated with outcomes. We select three features that are most
correlated with each of five predicted classes (each will be converted to
separate 0/1 variable to calculate numeric correlations), and get 12
correlated variables.

```{r, cache=TRUE}
nume<-tr[,1:53]
nume$A <- ifelse(tr$classe == "A", 1, 0)
nume$B <- ifelse(tr$classe == "B", 1, 0)
nume$C <- ifelse(tr$classe == "C", 1, 0)
nume$D <- ifelse(tr$classe == "D", 1, 0)
nume$E <- ifelse(tr$classe == "E", 1, 0)
nume.corr <- cor(nume)
corA <- nume.corr[54,1:53] %>% sort(decr=TRUE) %>% head(3) %>% names
corB <- nume.corr[55,1:53] %>% sort(decr=TRUE) %>% head(3) %>% names
corC <- nume.corr[56,1:53] %>% sort(decr=TRUE) %>% head(3) %>% names
corD <- nume.corr[57,1:53] %>% sort(decr=TRUE) %>% head(3) %>% names
corE <- nume.corr[58,1:53] %>% sort(decr=TRUE) %>% head(3) %>% names
(correlated <- unique(c(corA, corB, corC, corD, corE)))
tr12 <- tr[, correlated]
tr12$classe <- tr$classe
```

It looks like execution time almost halved this way, while prediction accuracy
is similar. Supposedly Random Forest automatically discovers same feature
importance.

```{r}
set.seed(1)
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2)
system.time(mod5rcv5.12<-train(classe~., data=tr,method="rf", ntree=5, trControl=ctrl))
confusionMatrix(predict(mod5rcv5.12, testing),testing$classe)
# 15 sec (0.9861, 0.9927)
```

Another approach is to check if multiple sensor are even needed. What if we
predict just based on belt sensors?

```{r}
set.seed(1)
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2)
system.time(mod<-train(classe~., data=tr[, c(1,54)], method="rf", ntree=5, trControl=ctrl))
confusionMatrix(predict(mod, testing),testing$classe)
# 15 sec (0.9861, 0.9927)
```

To try another model,
we use GBM (Generalized Boosted Regression Modeling) method with low number of
trees to predict. We change number of trees and number of available features
to compare speed and accuracy.

```{r, cache=TRUE}
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2, verboseIter=TRUE)
set.seed(1)
system.time(fitGBM5 <- train(classe~.,data=tr[,c(1:5,54)],
                        method="gbm", trControl=ctrl))
confusionMatrix(predict(fitGBM5, testing), testing$classe)
set.seed(1)
system.time(fitGBM50 <- train(classe~.,data=tr[,2:54],
                        method="gbm", trControl=ctrl))
confusionMatrix(predict(fitGBM50, testing), testing$classe)
set.seed(1)
system.time(fitGBM <- train(classe~.,data=tr12,
                        method="gbm", n.trees=5, trControl=ctrl))
confusionMatrix(predict(fitGBM, testing), testing$classe)
# failed
```

It is much faster - takes 37 seconds to train on a
desktop PC and we get prediction accuracy
in the interval 99.15%-99.65% which is very close to benchmark set for this
dataset.

We use Random Forest to improve weak trees and find that with default settings
accuracy is high: 99.68-99.93%.



Try to use less variables to make it work faster

```{r}
system.time(mod42<-train(classe~., data=tr[,c(2,3,4,5,55)],method="rf", ntree=3))
confusionMatrix(predict(mod42, testing),testing$classe)
# 12 sec,  99.6-99.89%
```

But why these four?

```{r}
pps<-prcomp(tr[, -c(1,55)], scale=T)
sort(pps$rotation[,1]) %>% tail(4) %>% names -> impacting
mod47<-train(classe~., data=tr[,c(impacting,"classe")],method="rf", ntree=3)
confusionMatrix(predict(mod47, testing),testing$classe)
# Still only 63%
```

Try to scale
```{r}
scaled <- sapply(tr[,-c(1,55)],scale) %>% as.data.frame
tr5<-scaled
tr5$classe<-tr$classe
```

Try neuralnet

```{r}
tr6<-tr[,-c(1,55)]
ftml<-xyform("cl",names(tr6))
tr6$cl<-ifelse(tr$classe=="A",1,0)
nnA<-neuralnet(ftml,tr6,hidden=5,lifesign="full",thre=0.7)
tr6$cl<-ifelse(tr$classe=="B",1,0)
nnB<-neuralnet(ftml,tr6,hidden=5,lifesign="full",thre=0.7)
tr6$cl<-ifelse(tr$classe=="C",1,0)
nnC<-neuralnet(ftml,tr6,hidden=5,lifesign="full",thre=0.5)
tr6$cl<-ifelse(tr$classe=="D",1,0)
nnD<-neuralnet(ftml,tr6,hidden=5,lifesign="full",thre=0.7)
tr6$cl<-ifelse(tr$classe=="E",1,0)
nnE<-neuralnet(ftml,tr6,hidden=5,lifesign="full",thre=0.7)

testingNotNA<-testing[,names(nas[nas == 0])]
ts<-testingNotNA[,-(1:6)]
ts6<-ts[,-c(54)]
#ts6$cl<-ifelse(ts$classe=="A",1,0)
#res<-ifelse(neuralnet::compute(nn, ts6)$net.result >0.5,1,0)
#confusionMatrix(res,ts6$cl)
# 82% of determining if it's A
#isA<-neuralnet::compute(nnA, ts6)$net.result
#isB<-neuralnet::compute(nnB, ts6)$net.result
#isC<-neuralnet::compute(nnC, ts6)$net.result
#isD<-neuralnet::compute(nnD, ts6)$net.result
#isE<-neuralnet::compute(nnE, ts6)$net.result
isA<-scale(neuralnet::compute(nnA, ts6)$net.result)
isB<-scale(neuralnet::compute(nnB, ts6)$net.result)
isC<-scale(neuralnet::compute(nnC, ts6)$net.result)
isD<-scale(neuralnet::compute(nnD, ts6)$net.result)
isE<-scale(neuralnet::compute(nnE, ts6)$net.result)
projs<-cbind(isA,isB,isC,isD,isE)
projs<-as.data.table(projs)
setnames(projs,names(projs),c("A","B","C","D","E"))
rrr<-matrix(NA,ncol=1)
for(i in 1:length(projs$A)) rrr<-cbind(rrr,names(which.max(projs[i,])))
#table(rrr[-1],ts$classe)
confusionMatrix(rrr[-1],ts$classe)
# Why C sensitivity is sooo low?
# 40%
confusionMatrix(ifelse(isA >0,1,0),ifelse(ts$classe=="A",1,0))
# 80 %
confusionMatrix(ifelse(isB >0,1,0),ifelse(ts$classe=="B",1,0))
# 81 %
confusionMatrix(ifelse(isC >0,1,0),ifelse(ts$classe=="C",1,0))
# 81 %
```

```{r, eval=FALSE}
unique(training[training$user_name=="jeremy","num_window"])
print(qplot(x=training$X,y=training$num_window,color=training$classe))
```

### Appendices

## Appendix 1. References

Dataset source is  
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.