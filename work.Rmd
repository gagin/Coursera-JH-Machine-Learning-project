---
title: "Application of machine learning algorithms to weightlifting sensor data"
author: "Alex Gaggin"
date: "November 18, 2015"
output: html_document
---

### Summary

[A dataset](http://groupware.les.inf.puc-rio.br/har) analyzed in this paper
includes sensor data from six subjects executing same weightlifting excersize
in five styles, only one of which is correct. It's shown how machine learning
can be used two interpret sensor data to recognize exercise styles. Practical
application for this kind of technology would be a programmatic trainer that
advises a trainee to correct their exercise form.
This particular paper though is done as a course project for Practical Machine
Learning class
in Data Science specialization on Coursera and is focused on applying machine
learning methods to the dataset.

While it will be shown that we do not need machine learning algorithms to generate
correct prediction for testing dataset provided by Coursera, sensor data
can still be used to recognize weightlifting exercise styles with high degree
of accuracy, and computing time for the learning process required to reach
benchmark accuracy rate (the original article quotes 98.03%) is quite brief.

### Exploratory analysis

```{r, include=FALSE}
# setwd(file.path(normalizePath("~"),"kaggle","predmachlearn-034"))
library(caret)
library(dplyr)
library(gridExtra)
```

We download the training dataset and split it to three parts - 60% training,
20% development test, 20% validation of out-of-sample error. The selected
algorithm will be later re-trained on the full set of training data before
making predictions on the testing part of the dataset that will be downloaded
separately and submitted as a second part of this project assignment.

```{r, cache=TRUE}
remote <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
dl.read <- function(remote) {
        loc <- basename(remote)
        if(!file.exists(loc )) download.file(remote, loc)
        read.csv(loc)}
working <- dl.read(remote)
set.seed(1000)
inTrain  <- createDataPartition(working$classe, p = 0.6, list=FALSE)
training <- working[inTrain, ]
nontraining <- working[-inTrain, ]
inTest <- createDataPartition(nontraining$classe, p = 0.5, list=FALSE)
testing  <- nontraining[inTest, ]
validating  <- nontraining[-inTest, ]
dim(working)
dim(training)
dim(testing)
dim(validating)
````

But visually reviewing data via View(), it can be seen that the data is grouped
to windows identified by common "num_window" variable
where the last entry has new_window==yes and then it has
more non-empty variables associated with it then observations with
new_window==no.These
ones have a lot of NAs, and "Div/0!", and empty values instead.

We select a short example of the data to illustrate this structure here.
We keep in mind that in the training set some rows are taken out for
verification sub-sets while selecting it, and remember that so the actual window
is probably longer, and there are more variables that shown in the example.

```{r, cache=TRUE}
finished <- training[training$new_window == "yes", "num_window"]
have.prev <- finished[((finished-1) %in% finished)]
length <- training[training$num_window %in% have.prev,
                                "num_window"] %>% table %>% .[.>1] %>% "["(1)
last.obs <- training %>% with(new_window == "yes" &
                                      num_window == names(length)) %>% which
range.obs <- (last.obs - length - 1):(last.obs + 1)
training[range.obs,c(1,2,6,7,8,12,17,18,160)]
```

Because they go last, we assume that the last observation in each window contains
some aggregate information for the whole window. Variable names agree to this
assumption. Supposedly actual application (programmatic trainer) could rely
on this aggregates for decisions, but for the purpose of this paper we ignore
these additional variables and will only keep ones that have values for every
observation.

By the way, all variables of each window are identified  as the same class
(calculation to verify this statement is shown below). Apparently,
because each window is an example of particular weightlifting exercise form.

```{r, cache=TRUE}
# A cross table of window number vs response class, only one class for each
# window has values, and four others are zeroes.
training$num_window %>% table(training$classe) %>% "=="(0) %>% rowSums %>%
        "!="(4) %>% sum

larger <- training %>% with(qplot(X, num_window, color=classe,
    main="Observation grouping per prediction\n class in the training sub-set"))
smaller <- training[training$X %in% 5050:5150, ] %>%
    with(qplot(X, num_window, color=classe,
    main="Zoomed: each point is actually\n a horizonal row of points"))
grid.arrange(larger, smaller, ncol=2)
```

Thus, as Coursera's assignment provides in its testing set the num_window
variable for the same users in the same range (so, from the same experiment),
we can get answers without using machine learning, but with direct lookup by
window number instead. We get these with the goal to use them as verification
for machine learning predictions based on sensor data.

```{r, cache=TRUE}
remote <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
assignment <- dl.read(remote)
# We use the full original training data set just in case of caret completely
# missing some window in the training sample after partition
classe.by.window <- function(window) {
        working$classe[which(working$num_window == window)[1]]}
(answers <- sapply(assignment$num_window,classe.by.window))
````

### Data cleanup

We now also remove non-sensor measurements: line numbers, user names,
timestamps and window markers, and aggreate variables. We are left with 52
predictor variables.

```{r, cache=TRUE}
nas<-sapply(training, function(x) sum(is.na(x) | x == "" | x == "#DIV/0!"))
trainingNotNA<-training[,names(nas[nas == 0])]
tr<-trainingNotNA[,-(1:7)]
dim(tr)
```

### Training with Recursive Partitioning

Because we have categorical outcomes, we can't use linear models, but turn
to classification trees instead.
We try to build simple decision tree and see that it fails to provide
a prediction much better than a coin flip. We use accuracy rate and training
time as two parameters we are interested in.

```{r, cache=TRUE}
set.seed(1)
system.time(
        confusionMatrix(
                predict(train(classe~., data=tr, method="rpart"), testing),
                testing$classe)$overall[1] %>% print)
```

### Training with Random Forest

As simple tree failed, we now try to use Random Forest as an algorithm
that is known for prediction accuracy. Because it's also known to be slow,
we first try it with low number of trees to speed the check up. We see that
accuracy is quite high, and so with higher number of trees we expect to get
result close to the benchmark.

```{r, cache=TRUE}
set.seed(1)
try.rf <- function(ctl, trees=5) {
        system.time(
                mod <- train(classe~., data=tr, method="rf",
                             ntree=trees, trControl=ctl)) %>% print       
        confusionMatrix(predict(mod, testing),
                        testing$classe)$overall[1] %>% print
        return(mod)
        }
trainControl() %>% try.rf -> mod5
```

### Optimizing train control for the Random Forest

Before running the long training, we try to optimize different parameters
for train control. For the speed, we keep
number of trees low. We realize that we don't know if the comparison results
depend on number of trees, but for the speed we presume they are.

After running tests with various train control methods, we find "repeatedcv",
"adaptive_boot", and "adaptive_LGOCV" methods to work better than others in
terms of accuracy/speed balance, but the best method is "out-of-bag" - it
provides good accuracy,  while being dramatically faster, and its accuracy
to increase in number of trees stronger. We will use it further.

```{r, cache=TRUE}
set.seed(1)
trainControl(method="repeatedcv", number=5, repeats=2) %>% try.rf -> mod5rcv5.2

set.seed(1)
trainControl(method="repeatedcv", number=10, repeats=2) %>% try.rf -> mod5rcv10.2

set.seed(1)
trainControl(method="repeatedcv", number=5, repeats=5) %>% try.rf -> mod5rcv5.5

set.seed(1)
trainControl(method="repeatedcv", number=20, repeats=2) %>% try.rf -> mod5rcv20.2

set.seed(1)
trainControl(method="cv", number=5) %>% try.rf -> mod5cv5

set.seed(1)
trainControl(method="oob") %>% try.rf -> mod5oob

set.seed(1)
trainControl(method="boot632") %>% try.rf -> mod5b6

set.seed(1)
trainControl(method="adaptive_boot") %>% try.rf -> mod5ab

set.seed(1)
trainControl(method="adaptive_cv") %>% try.rf -> mod5acv

set.seed(1)
trainControl(method="LGOCV") %>% try.rf -> mod5lgocv

set.seed(1)
trainControl(method="adaptive_LGOCV") %>% try.rf -> mod5algocv

set.seed(1)
trainControl(method="oob") %>% try.rf(10) -> mod10oob

trainControl(method="adaptive_boot") %>% try.rf(10) -> mod10b

trainControl(method="adaptive_LGOCV") %>% try.rf(10) -> mod10algocv

ctl <- trainControl(method="oob")

```

Just out of curiocity, we check fastest model against our answers and see that
just a single answer out of 20 is misclassified.

```{r, cache=TRUE}
confusionMatrix(predict(mod5oob, assignment), answers)
```

### Optimizing by reduction of predictors

We try to use reduction of predictors via  Principal Component pre-processing
Analysis and see that PCA doesn't improve either accuracy or speed.

```{r, cache=TRUE}
set.seed(1)
system.time(
        mod5PCA <- train(classe~., data=tr, method="rf",
                        ntree=5, trControl=ctl, preProcess="pca", thresh=0.8)
                        ) %>% print       
confusionMatrix(predict(mod5PCA, testing),
                        testing$classe)$overall[1] %>% print
```

We try manual selection of predictors with
highest correlation to outcomes (after making the outcomes separate 0/1
numeric variables). We see that thes increases speed but at a great
accuracy loss. We try to use speed gain to allow for more trees and folds
but it doesn't improve accuracy/speed balance either.

```{r, cache=TRUE}
nume<-tr[,1:52]
nume$A <- ifelse(tr$classe == "A", 1, 0)
nume$B <- ifelse(tr$classe == "B", 1, 0)
nume$C <- ifelse(tr$classe == "C", 1, 0)
nume$D <- ifelse(tr$classe == "D", 1, 0)
nume$E <- ifelse(tr$classe == "E", 1, 0)
nume.corr <- cor(nume)
corA <- nume.corr[53,1:52] %>% sort(decr=TRUE) %>% head(3) %>% names
corB <- nume.corr[54,1:52] %>% sort(decr=TRUE) %>% head(3) %>% names
corC <- nume.corr[55,1:52] %>% sort(decr=TRUE) %>% head(3) %>% names
corD <- nume.corr[56,1:52] %>% sort(decr=TRUE) %>% head(3) %>% names
corE <- nume.corr[57,1:52] %>% sort(decr=TRUE) %>% head(3) %>% names
(correlated <- unique(c(corA, corB, corC, corD, corE)))
tr12 <- tr[, correlated]
tr12$classe <- tr$classe

set.seed(1)
system.time(
        mod12 <- train(classe~., data=tr12, method="rf",
                        ntree=5, trControl=ctl)
                        ) %>% print       
confusionMatrix(predict(mod12, testing),
                        testing$classe)$overall[1] %>% print

set.seed(1)
system.time(
        mod12.20 <- train(classe~., data=tr12, method="rf",
                        ntree=20, trControl=ctl)
                        ) %>% print       
confusionMatrix(predict(mod12.20, testing),
                        testing$classe)$overall[1] %>% print
```

Another approach is to check if multiple sensor are even needed. We see that
while belt sensor data seems to provide better accuracy overall, but
any single sensor is worse than all of the together, and worse than predictors
selected by highest correlations to outcomes.

```{r, cache=TRUE}
tr.belt <- tr[, grepl("belt",names(tr),fixed=TRUE)]
tr.belt$classe <- tr$classe
set.seed(1)
system.time(
        mod.belt <- train(classe~., data=tr.belt, method="rf",
                        ntree=5, trControl=ctl)
                        ) %>% print       
confusionMatrix(predict(mod.belt, testing),
                        testing$classe)$overall[1] %>% print

tr.forearm <- tr[, grepl("forearm",names(tr),fixed=TRUE)]
tr.forearm$classe <- tr$classe
set.seed(1)
system.time(
        mod.forearm <- train(classe~., data=tr.forearm, method="rf",
                        ntree=5, trControl=ctl)
                        ) %>% print       
confusionMatrix(predict(mod.forearm, testing),
                        testing$classe)$overall[1] %>% print

tr.arm <- tr[, grepl("_arm",names(tr),fixed=TRUE)]
tr.arm$classe <- tr$classe
set.seed(1)
system.time(
        mod.arm <- train(classe~., data=tr.arm, method="rf",
                        ntree=5, trControl=ctl)
                        ) %>% print       
confusionMatrix(predict(mod.arm, testing),
                        testing$classe)$overall[1] %>% print

tr.dumbbell <- tr[, grepl("dumbbell",names(tr),fixed=TRUE)]
tr.dumbbell$classe <- tr$classe
set.seed(1)
system.time(
        mod.dumbbell <- train(classe~., data=tr.dumbbell, method="rf",
                        ntree=5, trControl=ctl)
                        ) %>% print       
confusionMatrix(predict(mod.dumbbell, testing),
                        testing$classe)$overall[1] %>% print
```

### Training with Generalized Boosted Regression

We try top use GBM (Generalized Boosted Regression Modeling) method but it
quite slow and doesn't improve accuracy.

```{r, cache=TRUE}
set.seed(1)
system.time(fitGBM <- train(classe~.,data=tr,
                        method="gbm", verbose=FALSE))
confusionMatrix(predict(fitGBM, testing), testing$classe)
```

### Out-of-sample accuracy prediction

We train model with Random Forest at high number of trees and with previously
found
cross-validation parameters. We apply resulting model to validation dataset
to infer out-of-sample accuracy assessment.

```{r}
set.seed(1)
system.time(
        ctl %>% try.rf(500) -> mod500oob
        ) %>% print
confusionMatrix(predict(mod500oob,validating),validating$classe)
```


Comparison to answers derived from num_window shows full match.

```{r}
confusionMatrix(predict(mod500oob, assignment), answers)$overall[1]
```



### Appendices

## Appendix 1. References

Dataset source is  
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.